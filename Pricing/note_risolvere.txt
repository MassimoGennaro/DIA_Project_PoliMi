DIA:
perchè context generation funziona male?
1: nuovo learner eredita t da vecchio, ma 
update incrementa t quando viene incrementato.
Se t aumenta di troppo, non esplora nuove opzioni?

2: devo printare la reward attesa 
per ogni arm per ogni contesto, e confrontarla con la
reward attesa ottimale dell'arm.
Potrei printare ansche la reward upper e lower bound.
In questa maniera, osservo accuratamente se la stima
converge verso il vero valore atteso.

3: perchè la reward è peggiore? perchè dopo lo split 
il rate di risposte ottime peggiora per 
poi migliorare.

4: comparare il context generation con il caso in cui
si parta già con 3 contesti, 
ciascuno corrispondente ad una diversa classe.
(dovrebbe essere molto migliore del modello disaggregato)
